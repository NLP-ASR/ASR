{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairseq_mod\n",
    "#import fairseq\n",
    "#from wav2vec2_inference_pipeline import inference_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "#from wav2vec2_inference_pipeline import inference_pipeline\n",
    "from data_loader import LibriSpeechDataLoader\n",
    "from knowledge_distillation.kd_training import KnowledgeDistillationTraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq_mod.models.wav2vec.teacher_wav2vec2 import TeacherWav2Vec2Model\n",
    "from fairseq_mod.models.wav2vec.student_wav2vec2 import StudentWav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq_mod.data.dictionary import Dictionary as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configurations and create letter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open('demo_config.yaml','r'), Loader=yaml.FullLoader)\n",
    "target_dict = D.load('ltr_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def postprocess_features(feats, sample_rate):\n",
    "    if feats.dim() == 2: feats = feats.mean(-1)\n",
    "    assert feats.dim() == 1, feats.dim()\n",
    "    with torch.no_grad():\n",
    "        feats = F.layer_norm(feats, feats.shape)\n",
    "    return feats\n",
    "\n",
    "def get_feature(batch_sample):\n",
    "    return postprocess_features(batch_sample[0][0], batch_sample[1])\n",
    "\n",
    "def get_padding_mask(batch_sample):\n",
    "    return torch.BoolTensor(batch_sample[0].size(1)).fill_(False)\n",
    "\n",
    "def get_batch_encoder_input(batch_samples):\n",
    "    features = [get_feature(batch_sample) for batch_sample in batch_samples]\n",
    "    features = torch.nn.utils.rnn.pad_sequence(features, batch_first=True, padding_value=0)\n",
    "    padding_masks = [get_padding_mask(batch_sample) for batch_sample in batch_samples]\n",
    "    padding_masks = torch.nn.utils.rnn.pad_sequence(padding_masks, batch_first=True, padding_value=True)\n",
    "    mask = False\n",
    "    features_only = True\n",
    "    return features, padding_masks, mask, features_only\n",
    "\n",
    "class LibriSpeechDataLoader:\n",
    "\n",
    "    \"\"\"\n",
    "    Data loaders for the LibriSpeech dataset.\n",
    "\n",
    "    Arguments:\n",
    "        train_batch_size (int): batch size for the training data loader\n",
    "        val_batch_size (int): batch size for the validation data loader\n",
    "        num_workers (int): number of workers for training and validation data loaders\n",
    "        train_data_path (str): Path to training data\n",
    "        val_data_path (str): Path to validation data\n",
    "        train_on_dev_clean (bool): Set to True if you want to train on parts of the dev-clean dataset and validate on the other part. This is useful when testing ideas\n",
    "        use_train_clean_100 (bool): Set to True if using LibriSpeech's train-clean-100 dataset during training\n",
    "        use_train_clean_360 (bool): Set to True if using LibriSpeech's train-clean-360 dataset during training\n",
    "        use_train_other_500 (bool): Set to True if using LibriSpeech's train-other-500 dataset during training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 download,\n",
    "                 train_batch_size,\n",
    "                 val_batch_size,\n",
    "                 num_workers,\n",
    "                 train_data_path,\n",
    "                 val_data_path,\n",
    "                 train_on_dev_clean,\n",
    "                 use_train_clean_100,\n",
    "                 use_train_clean_360,\n",
    "                 use_train_other_500,\n",
    "                 ):\n",
    "\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.download  = download\n",
    "        \n",
    "        dev_clean_dataset = torchaudio.datasets.LIBRISPEECH(val_data_path, url='dev-clean', download=False)\n",
    "        dev_other_dataset = torchaudio.datasets.LIBRISPEECH(val_data_path, url='dev-other', download=False)\n",
    "        dev_other_data_loader = DataLoader(dev_other_dataset,\n",
    "                                           batch_size = val_batch_size,\n",
    "                                           shuffle = False,\n",
    "                                           num_workers = num_workers)\n",
    "\n",
    "        if train_on_dev_clean:\n",
    "            train_data_loader, dev_train_data_loader, dev_clean_data_loader = self.create_data_loaders_from_dev_clean(dev_clean_dataset,\n",
    "                                                                                                                      train_batch_size,\n",
    "                                                                                                                      val_batch_size,\n",
    "                                                                                                                      num_workers)\n",
    "        else:\n",
    "            train_data_loader, dev_train_data_loader = self.create_data_loaders_from_train_dataset(train_data_path,\n",
    "                                                                                                   train_batch_size,\n",
    "                                                                                                   val_batch_size,\n",
    "                                                                                                   num_workers,\n",
    "                                                                                                   use_train_clean_100,\n",
    "                                                                                                   use_train_clean_360,\n",
    "                                                                                                   use_train_other_500,)\n",
    "            dev_clean_data_loader = DataLoader(dev_clean_dataset,\n",
    "                                               batch_size = val_batch_size,\n",
    "                                               shuffle = False,\n",
    "                                               num_workers = num_workers)\n",
    "\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.val_data_loaders = {\n",
    "                                 #\"dev_train\": dev_train_data_loader,\n",
    "                                 \"dev_clean\": dev_clean_data_loader,\n",
    "                                 #\"dev_other\": dev_other_data_loader\n",
    "                                }\n",
    "\n",
    "    def create_data_loaders_from_dev_clean(self,\n",
    "                                           dev_clean_dataset,\n",
    "                                           train_batch_size,\n",
    "                                           val_batch_size,\n",
    "                                           num_workers):\n",
    "\n",
    "        \"\"\"\n",
    "        Create train_data_loader and dev_train_data_loader from dev_clean_dataset.\n",
    "        Parts of dev_clean_dataset will be used for training, and the other part will be used for validating.\n",
    "\n",
    "        Arguments:\n",
    "            dev_clean_dataset (torchaudio.datasets.LIBRISPEECH): dev-clean data set from LibriSpeech\n",
    "            train_batch_size (int): batch size for the training data loader\n",
    "            val_batch_size (int): batch size for the validation data loader\n",
    "            num_workers (int): number of workers for the data loaders\n",
    "\n",
    "        Returns:\n",
    "            train_data_loader (torch.utils.data.DataLoader): data loader for training created from the dev clean dataset\n",
    "            dev_train_data_loader (torch.utils.data.DataLoader): data loader for validating created from the dev clean dataset\n",
    "        \"\"\"\n",
    "\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dev_clean_dataset,\n",
    "                                                                   [2203,500],\n",
    "                                                                   generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                        batch_size=train_batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        num_workers=num_workers,\n",
    "                                                        collate_fn=get_batch_encoder_input)\n",
    "        dev_train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                            batch_size=val_batch_size,\n",
    "                                                            shuffle=False,\n",
    "                                                            num_workers=num_workers,\n",
    "                                                            sampler=torch.utils.data.sampler.SubsetRandomSampler(torch.randint(high=2203, size=(500,))),)\n",
    "        dev_clean_data_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                            batch_size=val_batch_size,\n",
    "                                                            shuffle=False,\n",
    "                                                            num_workers=num_workers)\n",
    "        return train_data_loader, dev_train_data_loader, dev_clean_data_loader\n",
    "\n",
    "\n",
    "    def create_data_loaders_from_train_dataset(self,\n",
    "                                               train_data_path,\n",
    "                                               train_batch_size,\n",
    "                                               val_batch_size,\n",
    "                                               num_workers,\n",
    "                                               use_train_clean_100,\n",
    "                                               use_train_clean_360,\n",
    "                                               use_train_other_500):\n",
    "        \"\"\"\n",
    "        Create train_data_loader and dev_train_data_loader from training datasets of LibriSpeech.\n",
    "        Create the joint training dataset based on user's selections.\n",
    "\n",
    "        Arguments:\n",
    "            train_data_path (str): path to LibriSpeech training data\n",
    "            train_batch_size (int): batch size for train_data_loader\n",
    "            val_batch_size (int): batch size for dev_traiin_data_loader\n",
    "            num_workers (int): number of workers for data loaders\n",
    "            use_train_clean_100 (bool): Set to True if using LibriSpeech's train-clean-100 dataset during training\n",
    "            use_train_clean_360 (bool): Set to True if using LibriSpeech's train-clean-360 dataset during training\n",
    "            use_train_other_500 (bool): Set to True if using LibriSpeech's train-other-500 dataset during training\n",
    "\n",
    "        Returns:\n",
    "            train_data_loader (torch.utils.data.DataLoader): data loader for training created from LibriSpeech training datasets\n",
    "            dev_train_data_loader (torch.utils.data.DataLoader): data loader for validating created from LibriSpeech training datasets\n",
    "        \"\"\"\n",
    "        selected_datasets = []\n",
    "        if use_train_clean_100: selected_datasets.append(torchaudio.datasets.LIBRISPEECH(train_data_path, url='train-clean-100', download=self.download))\n",
    "        if use_train_clean_360: selected_datasets.append(torchaudio.datasets.LIBRISPEECH(train_data_path, url='train-clean-360', download=self.download))\n",
    "        if use_train_other_500: selected_datasets.append(torchaudio.datasets.LIBRISPEECH(train_data_path, url='train-other-500', download=self.download))\n",
    "        train_dataset = torch.utils.data.ConcatDataset(selected_datasets)\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                        batch_size=train_batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        num_workers=num_workers,\n",
    "                                                        collate_fn=get_batch_encoder_input)\n",
    "        dev_train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                            batch_size=val_batch_size,\n",
    "                                                            shuffle=False,\n",
    "                                                            num_workers=num_workers,\n",
    "                                                            sampler=torch.utils.data.sampler.SubsetRandomSampler(torch.randint(high=len(train_data_loader), size=(2000,))),)\n",
    "        return train_data_loader, dev_train_data_loader\n",
    "\n",
    "    def get_train_data_loader(self):\n",
    "        return self.train_data_loader\n",
    "\n",
    "    def get_val_data_loaders(self):\n",
    "        return self.val_data_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_trans_layer_student_model': 4,\n",
       " 'num_trans_layer_student_init_model': 12,\n",
       " 'student_init_model_path': '/common/home/vk405/Projects/Data/Data_wav2vec/wav2vec_big_960h.pt',\n",
       " 'student_init_model_type': 'fairseq_pretrained',\n",
       " 'student_trans_layer_init_method': 'every_k',\n",
       " 'change_conv_layers': True,\n",
       " 'conv_groups': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"knowledge_distillation\"][\"student_model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "libriSpeech_data_loader = LibriSpeechDataLoader(**config[\"data_loader\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = libriSpeech_data_loader.get_train_data_loader()\n",
    "val_data_loaders = libriSpeech_data_loader.get_val_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 241440])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference pipeline for validating the student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_pipeline_example = inference_pipeline(target_dict, use_cuda=True, input_half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 15:01:01 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/common/home/vk405/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /common/home/vk405/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/common/home/vk405/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/common/home/vk405/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /common/home/vk405/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/common/home/vk405/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/common/home/vk405/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/common/home/vk405/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "student_model = StudentWav2Vec2Model.create_student_model(target_dict=target_dict,\n",
    "                                                          fairseq_pretrained_model_path=config[\"knowledge_distillation\"][\"general\"][\"fairseq_pretrained_model_path\"],\n",
    "                                                          **config[\"knowledge_distillation\"][\"student_model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'feature_extractor.conv_layers.0.0.weight'.split('.')[2] != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['mask_emb', 'feature_extractor.conv_layers.0.0.weight', 'feature_extractor.conv_layers.0.2.weight', 'feature_extractor.conv_layers.0.2.bias', 'feature_extractor.conv_layers.1.0.weight', 'feature_extractor.conv_layers.2.0.weight', 'feature_extractor.conv_layers.3.0.weight', 'feature_extractor.conv_layers.4.0.weight', 'feature_extractor.conv_layers.5.0.weight', 'feature_extractor.conv_layers.6.0.weight', 'post_extract_proj.weight', 'post_extract_proj.bias', 'quantizer.vars', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_q.weight', 'project_q.bias', 'encoder.pos_conv.0.bias', 'encoder.pos_conv.0.weight_g', 'encoder.pos_conv.0.weight_v', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layer_norm.bias', 'layer_norm.weight', 'layer_norm.bias', 'final_proj.weight', 'final_proj.bias'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = list(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extrac_mod = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mod = list(feature_extrac_mod.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "   (1): Dropout(p=0.0, inplace=False)\n",
       "   (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "   (3): GELU()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "   (1): Dropout(p=0.0, inplace=False)\n",
       "   (2): GELU()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "   (1): Dropout(p=0.0, inplace=False)\n",
       "   (2): GELU()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "   (1): Dropout(p=0.0, inplace=False)\n",
       "   (2): GELU()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "   (1): Dropout(p=0.0, inplace=False)\n",
       "   (2): GELU()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "   (1): Dropout(p=0.0, inplace=False)\n",
       "   (2): GELU()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "   (1): Dropout(p=0.0, inplace=False)\n",
       "   (2): GELU()\n",
       " )]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(feat_mod[0].children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create student and teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TeacherWav2Vec2Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9de718f6bca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m teacher_model = TeacherWav2Vec2Model.create_teacher_model(target_dict=target_dict,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                           fairseq_pretrained_model_path=config[\"knowledge_distillation\"][\"general\"][\"fairseq_pretrained_model_path\"])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TeacherWav2Vec2Model' is not defined"
     ]
    }
   ],
   "source": [
    "teacher_model = TeacherWav2Vec2Model.create_teacher_model(target_dict=target_dict,\n",
    "                                                          fairseq_pretrained_model_path=config[\"knowledge_distillation\"][\"general\"][\"fairseq_pretrained_model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StudentWav2Vec2Model(\n",
      "  (feature_extractor): ConvFeatureExtractionModel(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "        (3): GELU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): GELU()\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "        (2): GELU()\n",
      "      )\n",
      "    )\n",
      "    (max_pool_1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (max_pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (dropout_input): Dropout(p=0.1, inplace=False)\n",
      "  (dropout_features): Dropout(p=0.1, inplace=False)\n",
      "  (quantizer): GumbelVectorQuantizer(\n",
      "    (weight_proj): Linear(in_features=512, out_features=640, bias=True)\n",
      "  )\n",
      "  (project_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (pos_conv): Sequential(\n",
      "      (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "      (1): SamePad()\n",
      "      (2): GELU()\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerSentenceEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dequantizer): DeQuantize()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerSentenceEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dequantizer): DeQuantize()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerSentenceEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dequantizer): DeQuantize()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerSentenceEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dequantizer): DeQuantize()\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (dropout3): Dropout(p=0.0, inplace=False)\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (final_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for module in student_model.modules():\n",
    "    print(module)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "l = nn.Sequential(nn.Conv1d(1,512,5),nn.Conv1d(512,512,5),nn.Conv1d(512,512,5),\\\n",
    "    nn.Conv1d(512,512,5),nn.Conv1d(512,512,5),nn.Conv1d(512,512,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "| Modules | Parameters |\n",
      "+---------+------------+\n",
      "|  weight |    1000    |\n",
      "+---------+------------+\n",
      "Total Trainable Params: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Conv1d(20,10,5,bias=False)\n",
    "dl = nn.Conv1d(20,10,5,groups=2,bias=False)\n",
    "count_parameters(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 5])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.state_dict()['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.state_dict()['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyr_ids = np.arange(0,20,2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 5])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.state_dict()['weight'][:,lyr_ids,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl.state_dict()['weight'].copy_(l.state_dict()['weight'][:,lyr_ids,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 5])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "| Modules | Parameters |\n",
      "+---------+------------+\n",
      "|  weight |    250     |\n",
      "+---------+------------+\n",
      "Total Trainable Params: 250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+------------+\n",
      "|                   Modules                    | Parameters |\n",
      "+----------------------------------------------+------------+\n",
      "|                   mask_emb                   |    1024    |\n",
      "|   feature_extractor.conv_layers.0.0.weight   |    5120    |\n",
      "|   feature_extractor.conv_layers.0.2.weight   |    512     |\n",
      "|    feature_extractor.conv_layers.0.2.bias    |    512     |\n",
      "|   feature_extractor.conv_layers.1.0.weight   |   786432   |\n",
      "|   feature_extractor.conv_layers.2.0.weight   |   786432   |\n",
      "|   feature_extractor.conv_layers.3.0.weight   |   786432   |\n",
      "|   feature_extractor.conv_layers.4.0.weight   |   786432   |\n",
      "|   feature_extractor.conv_layers.5.0.weight   |   524288   |\n",
      "|   feature_extractor.conv_layers.6.0.weight   |   524288   |\n",
      "|           post_extract_proj.weight           |   524288   |\n",
      "|            post_extract_proj.bias            |    1024    |\n",
      "|                quantizer.vars                |   245760   |\n",
      "|         quantizer.weight_proj.weight         |   327680   |\n",
      "|          quantizer.weight_proj.bias          |    640     |\n",
      "|               project_q.weight               |   589824   |\n",
      "|                project_q.bias                |    768     |\n",
      "|           encoder.pos_conv.0.bias            |    1024    |\n",
      "|         encoder.pos_conv.0.weight_g          |    128     |\n",
      "|         encoder.pos_conv.0.weight_v          |  8388608   |\n",
      "|   encoder.layers.0.self_attn.k_proj.weight   |  1048576   |\n",
      "|    encoder.layers.0.self_attn.k_proj.bias    |    1024    |\n",
      "|   encoder.layers.0.self_attn.v_proj.weight   |  1048576   |\n",
      "|    encoder.layers.0.self_attn.v_proj.bias    |    1024    |\n",
      "|   encoder.layers.0.self_attn.q_proj.weight   |  1048576   |\n",
      "|    encoder.layers.0.self_attn.q_proj.bias    |    1024    |\n",
      "|  encoder.layers.0.self_attn.out_proj.weight  |  1048576   |\n",
      "|   encoder.layers.0.self_attn.out_proj.bias   |    1024    |\n",
      "| encoder.layers.0.self_attn_layer_norm.weight |    1024    |\n",
      "|  encoder.layers.0.self_attn_layer_norm.bias  |    1024    |\n",
      "|         encoder.layers.0.fc1.weight          |  4194304   |\n",
      "|          encoder.layers.0.fc1.bias           |    4096    |\n",
      "|         encoder.layers.0.fc2.weight          |  4194304   |\n",
      "|          encoder.layers.0.fc2.bias           |    1024    |\n",
      "|   encoder.layers.0.final_layer_norm.weight   |    1024    |\n",
      "|    encoder.layers.0.final_layer_norm.bias    |    1024    |\n",
      "|   encoder.layers.1.self_attn.k_proj.weight   |  1048576   |\n",
      "|    encoder.layers.1.self_attn.k_proj.bias    |    1024    |\n",
      "|   encoder.layers.1.self_attn.v_proj.weight   |  1048576   |\n",
      "|    encoder.layers.1.self_attn.v_proj.bias    |    1024    |\n",
      "|   encoder.layers.1.self_attn.q_proj.weight   |  1048576   |\n",
      "|    encoder.layers.1.self_attn.q_proj.bias    |    1024    |\n",
      "|  encoder.layers.1.self_attn.out_proj.weight  |  1048576   |\n",
      "|   encoder.layers.1.self_attn.out_proj.bias   |    1024    |\n",
      "| encoder.layers.1.self_attn_layer_norm.weight |    1024    |\n",
      "|  encoder.layers.1.self_attn_layer_norm.bias  |    1024    |\n",
      "|         encoder.layers.1.fc1.weight          |  4194304   |\n",
      "|          encoder.layers.1.fc1.bias           |    4096    |\n",
      "|         encoder.layers.1.fc2.weight          |  4194304   |\n",
      "|          encoder.layers.1.fc2.bias           |    1024    |\n",
      "|   encoder.layers.1.final_layer_norm.weight   |    1024    |\n",
      "|    encoder.layers.1.final_layer_norm.bias    |    1024    |\n",
      "|   encoder.layers.2.self_attn.k_proj.weight   |  1048576   |\n",
      "|    encoder.layers.2.self_attn.k_proj.bias    |    1024    |\n",
      "|   encoder.layers.2.self_attn.v_proj.weight   |  1048576   |\n",
      "|    encoder.layers.2.self_attn.v_proj.bias    |    1024    |\n",
      "|   encoder.layers.2.self_attn.q_proj.weight   |  1048576   |\n",
      "|    encoder.layers.2.self_attn.q_proj.bias    |    1024    |\n",
      "|  encoder.layers.2.self_attn.out_proj.weight  |  1048576   |\n",
      "|   encoder.layers.2.self_attn.out_proj.bias   |    1024    |\n",
      "| encoder.layers.2.self_attn_layer_norm.weight |    1024    |\n",
      "|  encoder.layers.2.self_attn_layer_norm.bias  |    1024    |\n",
      "|         encoder.layers.2.fc1.weight          |  4194304   |\n",
      "|          encoder.layers.2.fc1.bias           |    4096    |\n",
      "|         encoder.layers.2.fc2.weight          |  4194304   |\n",
      "|          encoder.layers.2.fc2.bias           |    1024    |\n",
      "|   encoder.layers.2.final_layer_norm.weight   |    1024    |\n",
      "|    encoder.layers.2.final_layer_norm.bias    |    1024    |\n",
      "|   encoder.layers.3.self_attn.k_proj.weight   |  1048576   |\n",
      "|    encoder.layers.3.self_attn.k_proj.bias    |    1024    |\n",
      "|   encoder.layers.3.self_attn.v_proj.weight   |  1048576   |\n",
      "|    encoder.layers.3.self_attn.v_proj.bias    |    1024    |\n",
      "|   encoder.layers.3.self_attn.q_proj.weight   |  1048576   |\n",
      "|    encoder.layers.3.self_attn.q_proj.bias    |    1024    |\n",
      "|  encoder.layers.3.self_attn.out_proj.weight  |  1048576   |\n",
      "|   encoder.layers.3.self_attn.out_proj.bias   |    1024    |\n",
      "| encoder.layers.3.self_attn_layer_norm.weight |    1024    |\n",
      "|  encoder.layers.3.self_attn_layer_norm.bias  |    1024    |\n",
      "|         encoder.layers.3.fc1.weight          |  4194304   |\n",
      "|          encoder.layers.3.fc1.bias           |    4096    |\n",
      "|         encoder.layers.3.fc2.weight          |  4194304   |\n",
      "|          encoder.layers.3.fc2.bias           |    1024    |\n",
      "|   encoder.layers.3.final_layer_norm.weight   |    1024    |\n",
      "|    encoder.layers.3.final_layer_norm.bias    |    1024    |\n",
      "|          encoder.layer_norm.weight           |    1024    |\n",
      "|           encoder.layer_norm.bias            |    1024    |\n",
      "|              layer_norm.weight               |    512     |\n",
      "|               layer_norm.bias                |    512     |\n",
      "|              final_proj.weight               |   786432   |\n",
      "|               final_proj.bias                |    768     |\n",
      "+----------------------------------------------+------------+\n",
      "Total Trainable Params: 65456384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65456384"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(student_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the projection layer (which outputs probability distributions over tokens) for student and teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proj_layer(fairseq_pretrained_model_path):\n",
    "    \"\"\"\n",
    "    Get projection layer's weights and biases of wav2vec 2.0 pre-trained model\n",
    "    \"\"\"\n",
    "    w2v = torch.load(fairseq_pretrained_model_path)\n",
    "    return w2v[\"model\"][\"w2v_encoder.proj.weight\"], w2v[\"model\"][\"w2v_encoder.proj.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_layer_weight, proj_layer_bias = get_proj_layer(fairseq_pretrained_model_path=config[\"knowledge_distillation\"][\"general\"][\"fairseq_pretrained_model_path\"])\n",
    "student_model.init_proj_layer_to_decoder(proj_layer_weight, proj_layer_bias)\n",
    "teacher_model.init_proj_layer_to_decoder(proj_layer_weight, proj_layer_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a student model with knowledge distillation and get its performance on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/common/home/vk405/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ea23a611eb0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#inference_pipeline_example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m KD_wav2vec2 = KnowledgeDistillationTraining(train_data_loader = train_data_loader,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                             \u001b[0mval_data_loaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data_loaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                             \u001b[0minference_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                             \u001b[0mstudent_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Knowledge-Distillation-Toolkit/examples/wav2vec2_compression_demo/../../knowledge_distillation/kd_training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_gpu_used, max_epoch, optimize_method, scheduler_method, learning_rate, num_lr_warm_up_epoch, final_loss_coeff_dict, log_to_comet, comet_info_path, comet_exp_name, temperature, seed, track_grad_norm, accumulate_grad_batches, accelerator, num_nodes, precision, deterministic, resume_from_checkpoint, train_data_loader, val_data_loaders, inference_pipeline, student_model, teacher_model, logging_param)\u001b[0m\n\u001b[1;32m    228\u001b[0m                                        auto_histogram_weight_logging = True)\n\u001b[1;32m    229\u001b[0m             \u001b[0mcomet_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mcomet_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         self.trainer = Trainer(max_epochs = max_epoch,\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple_trainloader_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         self._accelerator_connector = AcceleratorConnector(\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_processes, devices, tpu_cores, ipus, accelerator, strategy, gpus, gpu_ids, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_training_type_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_distributed_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_given_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36mset_distributed_mode\u001b[0;34m(self, strategy)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m# finished configuring self._distrib_type, check ipython environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_interactive_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# for DDP overwrite nb processes by requested GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Crossmdl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36mcheck_interactive_compatibility\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_IS_INTERACTIVE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distrib_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distrib_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interactive_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             raise MisconfigurationException(\n\u001b[0m\u001b[1;32m    944\u001b[0m                 \u001b[0;34mf\"`Trainer(strategy={self._distrib_type.value!r})` or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0;34mf\" `Trainer(accelerator={self._distrib_type.value!r})` is not compatible with an interactive\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "#inference_pipeline_example\n",
    "KD_wav2vec2 = KnowledgeDistillationTraining(train_data_loader = train_data_loader,\n",
    "                                            val_data_loaders = val_data_loaders,\n",
    "                                            inference_pipeline = None,\n",
    "                                            student_model = student_model,\n",
    "                                            teacher_model = teacher_model,\n",
    "                                            num_gpu_used = config[\"knowledge_distillation\"][\"general\"][\"num_gpu_used\"],\n",
    "                                            temperature = config[\"knowledge_distillation\"][\"general\"][\"temperature\"],\n",
    "                                            final_loss_coeff_dict = config[\"knowledge_distillation\"][\"final_loss_coeff\"],\n",
    "                                            logging_param = ChainMap(config[\"knowledge_distillation\"][\"general\"], config[\"knowledge_distillation\"][\"optimization\"],\n",
    "                                                                     config[\"knowledge_distillation\"][\"final_loss_coeff\"], config[\"knowledge_distillation\"][\"student_model\"],\n",
    "                                                                     config[\"knowledge_distillation\"][\"pytorch_lightning_trainer\"]),\n",
    "                                            **ChainMap(config[\"knowledge_distillation\"][\"optimization\"],\n",
    "                                                       config[\"knowledge_distillation\"][\"pytorch_lightning_trainer\"],\n",
    "                                                       config[\"knowledge_distillation\"][\"comet_info\"])\n",
    "                                            )\n",
    "KD_wav2vec2.start_kd_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = KD_wav2vec2.get_student_model()\n",
    "val_result = inference_pipeline_example.run_inference_pipeline(student_model.cuda(), val_data_loaders[\"dev_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final WER is 48.40\n"
     ]
    }
   ],
   "source": [
    "print(\"final WER is {:.2f}\".format(val_result[\"inference_result\"]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the output above shows, WER has decreased from 99 to 48 after 5 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
